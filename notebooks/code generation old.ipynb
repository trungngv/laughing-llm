{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code generation for different models\n",
    "\n",
    "- Base model (phi1-5)\n",
    "- Teacher models\n",
    "    - GPT-4\n",
    "    - Code LLama (huggingface or openrouter)\n",
    "- Benchmark on human eval test set\n",
    "\n",
    "Test unit-tests generation for different models (maybe separate notebook)    \n",
    "- Require an evaluation metric for thoroughness of unit-test generation (e.g. out of U total tests that achieve 100% thorougness / coverage, how many are produced by the model + how many are functionally correct as well ... )\n",
    "\n",
    "- Base model\n",
    "- Teacher models\n",
    "- Model fine-tuned on code generation data\n",
    "- Model fine-tuned on code generation data with evol-instruct\n",
    "- Need to create own benchmark here or look for existing benchmark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation code generation ability\n",
    "\n",
    "This section evaluates some models on the HumanEval test set. The task by the model is to generate solution for a given problem description in the format of the HumanEval dataset.\n",
    "\n",
    "Models\n",
    "\n",
    "- Base model (phi1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model (phi1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 22:36:43.703209: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 22:36:44.820373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2023-09-22 22:36:44.823230: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2023-09-22 22:36:44.823246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\",\n",
    "                                             trust_remote_code=True, torch_dtype=\"auto\").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_eval.data import read_problems, write_jsonl\n",
    "problems = read_problems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(problems['HumanEval/0']['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response_phi15(response: str):\n",
    "    \"\"\"Extract the solution from phi1 model's response, as it often\n",
    "    generates some random function after the required solution was generated.\n",
    "\n",
    "    This could be improved further with more time.\n",
    "    \"\"\"\n",
    "    # discard the original prompt as it is included in the response\n",
    "    #response = response[len(prompt):]\n",
    "    \n",
    "    # get the result until the second def\n",
    "    def1_pos = response.index('def ')\n",
    "    try:\n",
    "        def2_pos = response.index('def ', def1_pos+4)\n",
    "    except ValueError as ex:\n",
    "        def2_pos = len(response)\n",
    "    return response[:def2_pos]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def mix_generate(model, tokenizer, prompt, max_new_tokens:int=512, num_sequences:int=10):\n",
    "  \"\"\"Generate output that is a mix of greedy and sampling.\n",
    "\n",
    "  The greedy approach seems to be the most effective, while beam-search\n",
    "  is not supported by the model. So, generate 1 sequence using greedy\n",
    "  and the rest using sampling.\n",
    "  \"\"\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
    "  seqs = []\n",
    "  # greedy generation\n",
    "  #outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "  #text = tokenizer.batch_decode(outputs)[0]\n",
    "  #seqs.append(parse_response_phi15(text))\n",
    "  # sampling generation\n",
    "  if num_sequences >= 1:\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True,\n",
    "                             do_sample=True, top_k=3, num_return_sequences=num_sequences)\n",
    "    seqs.extend([parse_response_phi15(text)\n",
    "                for text in tokenizer.batch_decode(outputs, skip_special_tokens=True)])\n",
    "\n",
    "  return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 0/1640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 320/1640 [16:33<1:06:22,  3.02s/it]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 560.00 MiB (GPU 0; 7.94 GiB total capacity; 6.69 GiB already allocated; 547.50 MiB free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code generation.ipynb 셀 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m task_id \u001b[39min\u001b[39;00m problems:\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m   p \u001b[39m=\u001b[39m problems[task_id][\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m   solutions \u001b[39m=\u001b[39m mix_generate(model, tokenizer, p,\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                            max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m   samples\u001b[39m.\u001b[39mextend([\u001b[39mdict\u001b[39m(task_id\u001b[39m=\u001b[39mtask_id, completion\u001b[39m=\u001b[39msolution) \u001b[39mfor\u001b[39;00m solution \u001b[39min\u001b[39;00m solutions])\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m   pbar\u001b[39m.\u001b[39mupdate(num_samples_per_task)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code generation.ipynb 셀 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# greedy generation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#text = tokenizer.batch_decode(outputs)[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#seqs.append(parse_response_phi15(text))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# sampling generation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mif\u001b[39;00m num_sequences \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m   outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens, use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m                            do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, top_k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49mnum_sequences)\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m   seqs\u001b[39m.\u001b[39mextend([parse_response_phi15(text)\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m               \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)])\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m seqs\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1649\u001b[0m         input_ids,\n\u001b[1;32m   1650\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1651\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1652\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1653\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1654\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1655\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1656\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1657\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1658\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1660\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2732\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2735\u001b[0m )\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/4a426d8015bef5a0cb3acff8d4474ee9ab4071d5/modeling_mixformer_sequential.py:773\u001b[0m, in \u001b[0;36mMixFormerSequentialForCausalLM.forward\u001b[0;34m(self, input_ids, labels, past_key_values, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    772\u001b[0m         hidden_layer \u001b[39m=\u001b[39m module(hidden_layer, past_cache\u001b[39m=\u001b[39mpast_key_values)\n\u001b[0;32m--> 773\u001b[0m     lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_layer)\n\u001b[1;32m    775\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/4a426d8015bef5a0cb3acff8d4474ee9ab4071d5/modeling_mixformer_sequential.py:655\u001b[0m, in \u001b[0;36mCausalLMHead.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mFloatTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m    654\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(hidden_states)\n\u001b[0;32m--> 655\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(hidden_states)\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m    657\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 560.00 MiB (GPU 0; 7.94 GiB total capacity; 6.69 GiB already allocated; 547.50 MiB free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "num_samples_per_task = 10\n",
    "max_new_tokens = 300\n",
    "\n",
    "start = time.time()\n",
    "pbar = tqdm(total=len(problems) * num_samples_per_task)\n",
    "samples = []\n",
    "for task_id in problems:\n",
    "  p = problems[task_id]['prompt']\n",
    "  solutions = mix_generate(model, tokenizer, p,\n",
    "                           max_new_tokens=max_new_tokens)\n",
    "  samples.extend([dict(task_id=task_id, completion=solution) for solution in solutions])\n",
    "  pbar.update(num_samples_per_task)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print('Total generation time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code generation.ipynb 셀 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://amlext%2B%2Bsubscriptions%2B75fd19db-8b39-4c55-bdcc-d9d449a6a79f%2Bresourcegroups%2Bml%2Bproviders%2Bmicrosoft.machinelearningservices%2Bworkspaces%2Btransformers%2Bcomputes%2Bgpu-nodes:public:d461c195-fa30-4388-bbd3-617c902f8dea/home/azureuser/cloudfiles/code/Users/trung.ngvan/laughing-llm/notebooks/code%20generation.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(samples)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "Response: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "\n",
      "\n",
      "=========================\n",
      "Prompt: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "\n",
      "Response: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "    \n",
      "=========================\n",
      "Prompt: \n",
      "\n",
      "\n",
      "def truncate_number(number: float) -> float:\n",
      "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
      "    and integer part (largest integer smaller than given number) and decimals\n",
      "    (leftover part always smaller than 1).\n",
      "\n",
      "    Return the decimal part of the number.\n",
      "    >>> truncate_number(3.5)\n",
      "    0.5\n",
      "    \"\"\"\n",
      "\n",
      "Response: \n",
      "\n",
      "\n",
      "def truncate_number(number: float) -> float:\n",
      "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
      "    and integer part (largest integer smaller than given number) and decimals\n",
      "    (leftover part always smaller than 1).\n",
      "\n",
      "    Return the decimal part of the number.\n",
      "    >>> truncate_number(3.5)\n",
      "    0.5\n",
      "    \"\"\"\n",
      "    integer_part, decimal_part = number.as_integer_ratio()\n",
      "    return decimal_part\n",
      "\n",
      "\n",
      "\n",
      "=========================\n",
      "Prompt: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def below_zero(operations: List[int]) -> bool:\n",
      "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
      "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
      "    at that point function should return True. Otherwise it should return False.\n",
      "    >>> below_zero([1, 2, 3])\n",
      "    False\n",
      "    >>> below_zero([1, 2, -4, 5])\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "Response: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def below_zero(operations: List[int]) -> bool:\n",
      "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
      "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
      "    at that point function should return True. Otherwise it should return False.\n",
      "    >>> below_zero([1, 2, 3])\n",
      "    False\n",
      "    >>> below_zero([1, 2, -4, 5])\n",
      "    True\n",
      "    \"\"\"\n",
      "    balance = 0\n",
      "    for operation in operations:\n",
      "        if operation < 0:\n",
      "            return True\n",
      "        balance += operation\n",
      "    return balance < 0\n",
      "\n",
      "\n",
      "\n",
      "=========================\n",
      "Prompt: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
      "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
      "    around the mean of this dataset.\n",
      "    Mean Absolute Deviation is the average absolute difference between each\n",
      "    element and a centerpoint (mean in this case):\n",
      "    MAD = average | x - x_mean |\n",
      "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
      "    1.0\n",
      "    \"\"\"\n",
      "\n",
      "Response: \n",
      "from typing import List\n",
      "\n",
      "\n",
      "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
      "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
      "    around the mean of this dataset.\n",
      "    Mean Absolute Deviation is the average absolute difference between each\n",
      "    element and a centerpoint (mean in this case):\n",
      "    MAD = average | x - x_mean |\n",
      "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
      "    1.0\n",
      "    \"\"\"\n",
      "    mean = sum(numbers) / len(numbers)\n",
      "    return sum(abs(x - mean) for x in numbers) / len(numbers)\n",
      "\n",
      "\n",
      "\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    p = problems[f'HumanEval/{i}']['prompt']\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True,\n",
    "                             do_sample=True, top_k=3)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    print(parse_response_phi15(text))\n",
    "    print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
